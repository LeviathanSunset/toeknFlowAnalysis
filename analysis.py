#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»£å¸æµåŠ¨å‡€æµå…¥/æµå‡ºåˆ†æžå·¥å…·
åˆ†æžæ¯ä¸ªåœ°å€çš„å‡€æµå…¥å’Œå‡€æµå‡ºæƒ…å†µ

ä½œè€…: LeviathanSunset
ç‰ˆæœ¬: 1.0.0
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from datetime import datetime

class TokenFlowAnalyzer:
    """ä»£å¸æµåŠ¨å‡€æµå…¥/æµå‡ºåˆ†æžå™¨"""
    
    def __init__(self, data_file=None):
        """
        åˆå§‹åŒ–åˆ†æžå™¨
        
        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.data_file = data_file
        self.df = None
        self.analysis_results = {}
        
    def load_data(self, file_path=None):
        """
        åŠ è½½æ•°æ®æ–‡ä»¶
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        if file_path:
            self.data_file = file_path
        
        if not self.data_file:
            # æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®æ–‡ä»¶
            storage_dir = Path("storage")
            if storage_dir.exists():
                json_files = list(storage_dir.glob("solscan_data_*.json"))
                if json_files:
                    self.data_file = max(json_files, key=lambda x: x.stat().st_mtime)
                    print(f"ðŸ” è‡ªåŠ¨é€‰æ‹©æœ€æ–°æ•°æ®æ–‡ä»¶: {self.data_file}")
                else:
                    raise FileNotFoundError("æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
            else:
                raise FileNotFoundError("storage ç›®å½•ä¸å­˜åœ¨")
        
        print(f"ðŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {self.data_file}")
        
        with open(self.data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if 'data' not in data:
            raise ValueError("æ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ 'data' å­—æ®µ")
        
        self.df = pd.DataFrame(data['data'])
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.df)} æ¡äº¤æ˜“è®°å½•")
        
        # æ•°æ®é¢„å¤„ç†
        self._preprocess_data()
        
    def _preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        print("ðŸ”§ æ•°æ®é¢„å¤„ç†ä¸­...")
        
        # è½¬æ¢æ•°æ®ç±»åž‹
        self.df['amount'] = pd.to_numeric(self.df['amount'], errors='coerce')
        self.df['value'] = pd.to_numeric(self.df['value'], errors='coerce')
        self.df['token_decimals'] = pd.to_numeric(self.df['token_decimals'], errors='coerce')
        self.df['block_time'] = pd.to_datetime(self.df['block_time'], unit='s', errors='coerce')
        
        # è®¡ç®—å®žé™…ä»£å¸æ•°é‡ï¼ˆå¤„ç†å°æ•°ä½ï¼‰
        self.df['actual_amount'] = self.df['amount'] / (10 ** self.df['token_decimals'])
        
        # èŽ·å–ä»£å¸åœ°å€ï¼ˆå‡è®¾æ‰€æœ‰äº¤æ˜“éƒ½æ˜¯åŒä¸€ä¸ªä»£å¸ï¼‰
        if 'token_address' in self.df.columns:
            token_address = self.df['token_address'].iloc[0]
            print(f"ðŸ” æ£€æµ‹åˆ°ä»£å¸åœ°å€: {token_address}")
            
            # å°è¯•ä»ŽSolscanAnalyzerèŽ·å–çœŸå®žçš„æ€»ä¾›åº”é‡
            try:
                from solscanCrawler import SolscanAnalyzer
                crawler = SolscanAnalyzer()
                metadata = crawler.get_token_metadata(token_address)
                
                if metadata and 'actual_total_supply' in metadata:
                    self.estimated_token_supply = metadata['actual_total_supply']
                    print(f"âœ… èŽ·å–çœŸå®žä»£å¸ä¾›åº”é‡: {self.estimated_token_supply:,.2f}")
                else:
                    # å¦‚æžœèŽ·å–å¤±è´¥ï¼Œä½¿ç”¨æ”¹è¿›çš„ä¼°ç®—æ–¹æ³•
                    print("âš ï¸ æ— æ³•èŽ·å–çœŸå®žä¾›åº”é‡ï¼Œä½¿ç”¨æ”¹è¿›ä¼°ç®—æ–¹æ³•")
                    
                    total_volume = self.df['actual_amount'].sum()
                    max_single_amount = self.df['actual_amount'].max()
                    unique_addresses = len(set(self.df['from_address'].unique()) | set(self.df['to_address'].unique()))
                    
                    # æ”¹è¿›çš„ä¼°ç®—é€»è¾‘ï¼š
                    # 1. åŸºäºŽè§‚å¯Ÿåˆ°çš„æœ€å¤§æŒä»“ä¼°ç®—
                    # 2. è€ƒè™‘åœ°å€æ•°é‡çš„å½±å“
                    # 3. ä¸ºmemeå¸å’Œpump.funä»£å¸è°ƒæ•´å‚æ•°
                    
                    if 'pump' in token_address.lower():
                        # pump.fun ä»£å¸é€šå¸¸ä¾›åº”é‡è¾ƒå¤§
                        estimated_multiplier = 50
                        print("ðŸŽ¯ æ£€æµ‹åˆ°pump.funä»£å¸ï¼Œä½¿ç”¨ä¸“ç”¨ä¼°ç®—å‚æ•°")
                    else:
                        estimated_multiplier = 20
                    
                    # å¤šç§ä¼°ç®—æ–¹æ³•å–æœ€å¤§å€¼ï¼Œç¡®ä¿ä¸ä¼šä½Žä¼°
                    estimates = [
                        max_single_amount * estimated_multiplier,  # åŸºäºŽæœ€å¤§å•ç¬”
                        total_volume * 10,  # åŸºäºŽæ€»äº¤æ˜“é‡
                        max_single_amount * unique_addresses * 0.5  # åŸºäºŽåœ°å€æ•°é‡
                    ]
                    
                    self.estimated_token_supply = max(estimates)
                    print(f"ðŸ“Š æ”¹è¿›ä¼°ç®—ä»£å¸ä¾›åº”é‡: {self.estimated_token_supply:,.2f}")
                    print(f"   ðŸ”¢ ä¼°ç®—ä¾æ®: æœ€å¤§å•ç¬” {max_single_amount:,.2f} Ã— {estimated_multiplier}")
                    print(f"   ðŸ‘¥ è§‚å¯Ÿåœ°å€æ•°: {unique_addresses}")
                    print(f"   ðŸ“ˆ æ€»äº¤æ˜“é‡: {total_volume:,.2f}")
                    
            except ImportError:
                print("âš ï¸ æ— æ³•å¯¼å…¥SolscanAnalyzerï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•")
                total_volume = self.df['actual_amount'].sum()
                max_single_amount = self.df['actual_amount'].max()
                self.estimated_token_supply = max(total_volume * 5, max_single_amount * 200)
                print(f"ðŸ“Š ä¼°ç®—ä»£å¸ä¾›åº”é‡: {self.estimated_token_supply:,.2f}")
        else:
            # å¦‚æžœæ²¡æœ‰token_addresså­—æ®µï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•
            total_volume = self.df['actual_amount'].sum()
            max_single_amount = self.df['actual_amount'].max()
            self.estimated_token_supply = max(total_volume * 5, max_single_amount * 200)
            print(f"ðŸ“Š ä¼°ç®—ä»£å¸ä¾›åº”é‡: {self.estimated_token_supply:,.2f}")
        
        print(f"ðŸ” è§‚å¯Ÿåˆ°çš„æ€»äº¤æ˜“é‡: {self.df['actual_amount'].sum():,.2f}")
        print(f"ðŸ’° æœ€å¤§å•ç¬”äº¤æ˜“: {self.df['actual_amount'].max():,.2f}")
        
        # å¤„ç†ç¼ºå¤±å€¼
        self.df = self.df.dropna(subset=['from_address', 'to_address', 'amount', 'actual_amount'])
        
        print(f"ðŸ’° ä»£å¸å°æ•°ä½: {self.df['token_decimals'].iloc[0]} ä½")
        print(f"ðŸ”¢ åŽŸå§‹æ•°é‡èŒƒå›´: {self.df['amount'].min():,.0f} - {self.df['amount'].max():,.0f}")
        print(f"ðŸª™ å®žé™…ä»£å¸æ•°é‡èŒƒå›´: {self.df['actual_amount'].min():,.6f} - {self.df['actual_amount'].max():,.6f}")
        print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆè®°å½•: {len(self.df)} æ¡")
    
    def calculate_net_flows(self):
        """
        è®¡ç®—æ¯ä¸ªåœ°å€çš„å‡€æµå…¥/æµå‡º
        
        Returns:
            dict: åŒ…å«å‡€æµå…¥/æµå‡ºåˆ†æžç»“æžœ
        """
        print("ðŸ“Š è®¡ç®—å‡€æµå…¥/æµå‡º...")
        
        # è®¡ç®—æµå‡ºï¼ˆä½œä¸ºå‘é€æ–¹ï¼‰- åŸºäºŽä»£å¸æ•°é‡
        outflows = self.df.groupby('from_address').agg({
            'actual_amount': 'sum',
            'amount': 'sum', 
            'value': 'sum',
            'trans_id': 'count'
        }).rename(columns={
            'actual_amount': 'outflow_tokens',
            'amount': 'outflow_raw_amount',
            'value': 'outflow_value',
            'trans_id': 'outflow_count'
        })
        
        # è®¡ç®—æµå…¥ï¼ˆä½œä¸ºæŽ¥æ”¶æ–¹ï¼‰- åŸºäºŽä»£å¸æ•°é‡
        inflows = self.df.groupby('to_address').agg({
            'actual_amount': 'sum',
            'amount': 'sum',
            'value': 'sum',
            'trans_id': 'count'
        }).rename(columns={
            'actual_amount': 'inflow_tokens',
            'amount': 'inflow_raw_amount', 
            'value': 'inflow_value',
            'trans_id': 'inflow_count'
        })
        
        # åˆå¹¶æ•°æ®ï¼Œåˆ›å»ºå®Œæ•´çš„åœ°å€åˆ—è¡¨
        all_addresses = set(self.df['from_address'].unique()) | set(self.df['to_address'].unique())
        
        # åˆ›å»ºå‡€æµåŠ¨åˆ†æžè¡¨
        net_flows = []
        
        for address in all_addresses:
            # èŽ·å–æµå…¥æ•°æ®
            inflow_tokens = inflows.loc[address, 'inflow_tokens'] if address in inflows.index else 0
            inflow_raw_amount = inflows.loc[address, 'inflow_raw_amount'] if address in inflows.index else 0
            inflow_value = inflows.loc[address, 'inflow_value'] if address in inflows.index else 0
            inflow_count = inflows.loc[address, 'inflow_count'] if address in inflows.index else 0
            
            # èŽ·å–æµå‡ºæ•°æ®
            outflow_tokens = outflows.loc[address, 'outflow_tokens'] if address in outflows.index else 0
            outflow_raw_amount = outflows.loc[address, 'outflow_raw_amount'] if address in outflows.index else 0
            outflow_value = outflows.loc[address, 'outflow_value'] if address in outflows.index else 0
            outflow_count = outflows.loc[address, 'outflow_count'] if address in outflows.index else 0
            
            # è®¡ç®—å‡€æµåŠ¨ï¼ˆåŸºäºŽä»£å¸æ•°é‡ï¼‰
            net_tokens = inflow_tokens - outflow_tokens
            net_value = inflow_value - outflow_value
            total_transactions = inflow_count + outflow_count
            
            # è®¡ç®—æ¯”çŽ‡ï¼ˆåŸºäºŽä»£å¸æ•°é‡ï¼‰
            if outflow_tokens > 0:
                flow_ratio = inflow_tokens / outflow_tokens
            elif inflow_tokens > 0:
                flow_ratio = float('inf')  # åªæœ‰æµå…¥ï¼Œæ— æµå‡º
            else:
                flow_ratio = 0  # æ— æ´»åŠ¨
            
            net_flows.append({
                'address': address,
                'inflow_tokens': inflow_tokens,
                'inflow_raw_amount': inflow_raw_amount,
                'inflow_value': inflow_value,
                'inflow_count': inflow_count,
                'outflow_tokens': outflow_tokens,
                'outflow_raw_amount': outflow_raw_amount,
                'outflow_value': outflow_value,
                'outflow_count': outflow_count,
                'net_tokens': net_tokens,
                'net_value': net_value,
                'total_transactions': total_transactions,
                'flow_ratio': flow_ratio,
                'address_type': self._classify_address_type(net_tokens, inflow_tokens, outflow_tokens, total_transactions)
            })
        
        self.net_flows_df = pd.DataFrame(net_flows)
        
        # æ˜¾ç¤ºåˆ†ç±»é˜ˆå€¼ä¿¡æ¯
        whale_threshold = self.estimated_token_supply * 0.001
        large_holder_threshold = self.estimated_token_supply * 0.0005
        medium_threshold = self.estimated_token_supply * 0.0001
        
        print(f"âœ… å®Œæˆ {len(self.net_flows_df)} ä¸ªåœ°å€çš„å‡€æµåŠ¨è®¡ç®—")
        print("\nðŸ·ï¸ åœ°å€åˆ†ç±»é˜ˆå€¼ (åŸºäºŽæµé€šé‡ç™¾åˆ†æ¯”):")
        print(f"   ðŸ“Š ä¼°ç®—ä»£å¸ä¾›åº”é‡: {self.estimated_token_supply:,.2f}")
        print(f"   ðŸ‹ é²¸é±¼é˜ˆå€¼ (0.1%): {whale_threshold:,.2f} ä»£å¸")
        print(f"   ðŸ¢ å¤§æˆ·é˜ˆå€¼ (0.05%): {large_holder_threshold:,.2f} ä»£å¸")  
        print(f"   ðŸ“ˆ ä¸­ç­‰é˜ˆå€¼ (0.01%): {medium_threshold:,.2f} ä»£å¸")
        
        return self.net_flows_df
    
    def _classify_address_type(self, net_tokens, inflow_tokens, outflow_tokens, total_transactions):
        """
        åˆ†ç±»åœ°å€ç±»åž‹ï¼ˆåŸºäºŽä»£å¸æµé€šé‡ç™¾åˆ†æ¯”ï¼‰
        
        Args:
            net_tokens: å‡€ä»£å¸æµåŠ¨
            inflow_tokens: æµå…¥ä»£å¸æ•°é‡
            outflow_tokens: æµå‡ºä»£å¸æ•°é‡
            total_transactions: æ€»äº¤æ˜“æ•°
        
        Returns:
            str: åœ°å€ç±»åž‹
        """
        # åŸºäºŽæµé€šé‡ç™¾åˆ†æ¯”çš„é˜ˆå€¼è®¾ç½®
        whale_threshold = self.estimated_token_supply * 0.001  # 0.1% æµé€šé‡
        large_holder_threshold = self.estimated_token_supply * 0.0005  # 0.05% æµé€šé‡
        medium_threshold = self.estimated_token_supply * 0.0001  # 0.01% æµé€šé‡
        
        active_threshold = 10    # 10ç¬”äº¤æ˜“
        
        # è®¡ç®—åœ°å€çš„æœ€å¤§æŒä»“å½±å“ï¼ˆæµå…¥æˆ–æµå‡ºçš„æœ€å¤§å€¼ï¼‰
        max_position = max(abs(inflow_tokens), abs(outflow_tokens))
        
        if total_transactions >= active_threshold:
            # é«˜é¢‘äº¤æ˜“è€…
            if abs(net_tokens) < (whale_threshold * 0.1):  # å‡€æµåŠ¨å¾ˆå°ï¼ˆé²¸é±¼é˜ˆå€¼çš„10%ï¼‰
                if max_position >= whale_threshold:
                    return "å¤§åž‹åšå¸‚å•†"
                else:
                    return "åšå¸‚å•†/å¥—åˆ©è€…"
            elif abs(net_tokens) >= whale_threshold:  # >= 0.1% æµé€šé‡
                if net_tokens > 0:
                    return "é²¸é±¼ä¹°å…¥"
                else:
                    return "é²¸é±¼å–å‡º"
            elif abs(net_tokens) >= large_holder_threshold:  # >= 0.05% æµé€šé‡
                if net_tokens > 0:
                    return "å¤§ä¹°å®¶"
                else:
                    return "å¤§å–å®¶"
            elif net_tokens > 0:
                return "æ´»è·ƒä¹°å®¶"
            else:
                return "æ´»è·ƒå–å®¶"
        else:
            # ä½Žé¢‘äº¤æ˜“è€…
            if abs(net_tokens) >= whale_threshold:  # >= 0.1% æµé€šé‡
                if net_tokens > 0:
                    return "é²¸é±¼ä¹°å…¥"
                else:
                    return "é²¸é±¼å–å‡º"
            elif abs(net_tokens) >= large_holder_threshold:  # >= 0.05% æµé€šé‡
                if net_tokens > 0:
                    return "å¤§æˆ·ä¹°å…¥"
                else:
                    return "å¤§æˆ·å–å‡º"
            elif abs(net_tokens) >= medium_threshold:  # >= 0.01% æµé€šé‡
                if net_tokens > 0:
                    return "ä¸­ç­‰ä¹°å®¶"
                else:
                    return "ä¸­ç­‰å–å®¶"
            elif net_tokens > 0:
                return "æ™®é€šä¹°å®¶"
            elif net_tokens < 0:
                return "æ™®é€šå–å®¶"
            else:
                return "æ— å‡€æµåŠ¨"
    
    def get_top_net_inflows(self, top_n=20):
        """
        èŽ·å–å‡€æµå…¥æœ€å¤§çš„åœ°å€
        
        Args:
            top_n: è¿”å›žå‰Nä¸ªåœ°å€
        
        Returns:
            pd.DataFrame: å‡€æµå…¥æŽ’è¡Œæ¦œ
        """
        if self.net_flows_df is None:
            self.calculate_net_flows()
        
        top_inflows = self.net_flows_df.nlargest(top_n, 'net_tokens')
        
        print("ðŸ† å‡€æµå…¥æœ€å¤§çš„åœ°å€ (Top 20) - æŒ‰ä»£å¸æ•°é‡:")
        print("=" * 140)
        print(f"{'æŽ’å':<4} {'åœ°å€':<20} {'å‡€æµå…¥(ä»£å¸)':<15} {'æµå…¥(ä»£å¸)':<15} {'æµå‡º(ä»£å¸)':<15} {'äº¤æ˜“æ•°':<8} {'ç±»åž‹':<12}")
        print("=" * 140)
        
        for idx, row in top_inflows.iterrows():
            rank = top_inflows.index.get_loc(idx) + 1
            address = row['address'][:16] + "..." if len(row['address']) > 16 else row['address']
            print(f"{rank:<4} {address:<20} {row['net_tokens']:<15,.6f} {row['inflow_tokens']:<15,.6f} "
                  f"{row['outflow_tokens']:<15,.6f} {row['total_transactions']:<8} {row['address_type']:<12}")
        
        return top_inflows
    
    def get_top_net_outflows(self, top_n=20):
        """
        èŽ·å–å‡€æµå‡ºæœ€å¤§çš„åœ°å€
        
        Args:
            top_n: è¿”å›žå‰Nä¸ªåœ°å€
        
        Returns:
            pd.DataFrame: å‡€æµå‡ºæŽ’è¡Œæ¦œ
        """
        if self.net_flows_df is None:
            self.calculate_net_flows()
        
        top_outflows = self.net_flows_df.nsmallest(top_n, 'net_tokens')
        
        print("\nðŸ“‰ å‡€æµå‡ºæœ€å¤§çš„åœ°å€ (Top 20) - æŒ‰ä»£å¸æ•°é‡:")
        print("=" * 140)
        print(f"{'æŽ’å':<4} {'åœ°å€':<20} {'å‡€æµå‡º(ä»£å¸)':<15} {'æµå…¥(ä»£å¸)':<15} {'æµå‡º(ä»£å¸)':<15} {'äº¤æ˜“æ•°':<8} {'ç±»åž‹':<12}")
        print("=" * 140)
        
        for idx, row in top_outflows.iterrows():
            rank = top_outflows.index.get_loc(idx) + 1
            address = row['address'][:16] + "..." if len(row['address']) > 16 else row['address']
            net_outflow = abs(row['net_tokens'])
            print(f"{rank:<4} {address:<20} {net_outflow:<15,.6f} {row['inflow_tokens']:<15,.6f} "
                  f"{row['outflow_tokens']:<15,.6f} {row['total_transactions']:<8} {row['address_type']:<12}")
        
        return top_outflows
    
    def analyze_address_patterns(self):
        """åˆ†æžåœ°å€è¡Œä¸ºæ¨¡å¼"""
        if self.net_flows_df is None:
            self.calculate_net_flows()
        
        print("\nðŸ“ˆ åœ°å€è¡Œä¸ºæ¨¡å¼åˆ†æž:")
        print("=" * 80)
        
        # æŒ‰ç±»åž‹ç»Ÿè®¡
        type_stats = self.net_flows_df['address_type'].value_counts()
        print("ðŸ·ï¸ åœ°å€ç±»åž‹åˆ†å¸ƒ:")
        for addr_type, count in type_stats.items():
            percentage = count / len(self.net_flows_df) * 100
            print(f"   {addr_type}: {count} ä¸ª ({percentage:.1f}%)")
        
        # å‡€æµå…¥/æµå‡ºç»Ÿè®¡
        total_net_inflow = self.net_flows_df[self.net_flows_df['net_value'] > 0]['net_value'].sum()
        total_net_outflow = abs(self.net_flows_df[self.net_flows_df['net_value'] < 0]['net_value'].sum())
        
        print(f"\nðŸ’° æ•´ä½“æµåŠ¨æ€§åˆ†æž:")
        print(f"   æ€»å‡€æµå…¥: ${total_net_inflow:,.2f}")
        print(f"   æ€»å‡€æµå‡º: ${total_net_outflow:,.2f}")
        print(f"   å‡€å·®é¢: ${total_net_inflow - total_net_outflow:,.2f}")
        
        # å¤§æˆ·åˆ†æžï¼ˆå‡€æµåŠ¨ > $1000ï¼‰
        whales = self.net_flows_df[abs(self.net_flows_df['net_value']) > 1000]
        whale_buyers = whales[whales['net_value'] > 0]
        whale_sellers = whales[whales['net_value'] < 0]
        
        print(f"\nðŸ‹ å¤§æˆ·åˆ†æž (å‡€æµåŠ¨ > $1,000):")
        print(f"   å¤§ä¹°å®¶æ•°é‡: {len(whale_buyers)} ä¸ª")
        print(f"   å¤§å–å®¶æ•°é‡: {len(whale_sellers)} ä¸ª")
        print(f"   å¤§ä¹°å®¶æ€»æµå…¥: ${whale_buyers['net_value'].sum():,.2f}")
        print(f"   å¤§å–å®¶æ€»æµå‡º: ${abs(whale_sellers['net_value'].sum()):,.2f}")
    
    def save_analysis_results(self, filename=None):
        """ä¿å­˜åˆ†æžç»“æžœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"storage/net_flow_analysis_{timestamp}.json"
        
        results = {
            "analysis_time": datetime.now().isoformat(),
            "total_addresses": len(self.net_flows_df),
            "net_flows": self.net_flows_df.to_dict('records'),
            "summary": {
                "total_net_inflow": float(self.net_flows_df[self.net_flows_df['net_value'] > 0]['net_value'].sum()),
                "total_net_outflow": float(abs(self.net_flows_df[self.net_flows_df['net_value'] < 0]['net_value'].sum())),
                "address_type_distribution": self.net_flows_df['address_type'].value_counts().to_dict()
            }
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(filename).parent.mkdir(parents=True, exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        # åŒæ—¶ä¿å­˜CSVæ ¼å¼
        csv_filename = filename.replace('.json', '.csv')
        self.net_flows_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        
        print(f"\nðŸ’¾ åˆ†æžç»“æžœå·²ä¿å­˜:")
        print(f"   JSON: {filename}")
        print(f"   CSV:  {csv_filename}")
        
        return filename
    
    def run_full_analysis(self, data_file=None):
        """è¿è¡Œå®Œæ•´çš„å‡€æµå…¥/æµå‡ºåˆ†æž"""
        print("ðŸŒŸ ä»£å¸æµåŠ¨å‡€æµå…¥/æµå‡ºåˆ†æžå¼€å§‹")
        print("=" * 80)
        
        # åŠ è½½æ•°æ®
        self.load_data(data_file)
        
        # è®¡ç®—å‡€æµåŠ¨
        self.calculate_net_flows()
        
        # åˆ†æžç»“æžœ
        print("\n" + "=" * 80)
        top_inflows = self.get_top_net_inflows(20)
        top_outflows = self.get_top_net_outflows(20)
        
        # åˆ†æžæ¨¡å¼
        self.analyze_address_patterns()
        
        # ä¿å­˜ç»“æžœ
        saved_file = self.save_analysis_results()
        
        print("\nðŸŽ‰ åˆ†æžå®Œæˆï¼")
        print(f"ðŸ“Š åˆ†æžäº† {len(self.net_flows_df)} ä¸ªåœ°å€")
        print(f"ðŸ“ ç»“æžœæ–‡ä»¶: {saved_file}")
        
        return {
            'net_flows': self.net_flows_df,
            'top_inflows': top_inflows,
            'top_outflows': top_outflows,
            'file': saved_file
        }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä»£å¸æµåŠ¨å‡€æµå…¥/æµå‡ºåˆ†æžå·¥å…·')
    parser.add_argument('--data', '-d', help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--top', '-t', type=int, default=20, help='æ˜¾ç¤ºæŽ’è¡Œæ¦œæ•°é‡')
    
    args = parser.parse_args()
    
    try:
        analyzer = TokenFlowAnalyzer()
        result = analyzer.run_full_analysis(args.data)
        
        if result:
            print("\nâœ¨ æ‰€æœ‰åˆ†æžå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ åˆ†æžå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
