#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Solscan Token Flow Analysis Tool
ä¸€ä½“åŒ–ä»£å¸æµåŠ¨åˆ†æå·¥å…·

åŠŸèƒ½åŒ…æ‹¬ï¼š
1. Solscan API æ•°æ®çˆ¬å–
2. è‡ªåŠ¨ cf_clearance æ›´æ–°
3. æ•°æ®åˆ†æå’Œè½¬æ¢
4. æ™ºèƒ½é˜²æŠ¤ç»•è¿‡

ä½œè€…: LeviathanSunset
ç‰ˆæœ¬: 2.0.0
"""

import requests
import json
import time
import yaml
import os
import pandas as pd
from datetime import datetime
from pathlib import Path
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class SolscanAnalyzer:
    """Solscan ä»£å¸æµåŠ¨åˆ†æå™¨ - ä¸€ä½“åŒ–å·¥å…·"""
    
    def __init__(self, config_path="settings/config.yaml"):
        self.config_path = config_path
        self.config = self._load_config(config_path)
        self.base_url = self.config['api']['base_url']
        self.session = requests.Session()
        self.cf_clearance_updated = False
        
        # è®¾ç½®ä»£ç†
        if self.config['proxy']['enabled']:
            self.proxies = {
                'http': self.config['proxy']['http'],
                'https': self.config['proxy']['https']
            }
        else:
            self.proxies = {}
        
        # è®¾ç½®é‡è¯•ç­–ç•¥
        retry_config = self.config['retry']
        retry_strategy = Retry(
            total=retry_config['max_retries'],
            backoff_factor=retry_config['backoff_factor'],
            status_forcelist=retry_config['status_codes'],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # è®¾ç½®è¯·æ±‚å¤´å’Œcookies
        self.headers = self._build_headers()
        self.cookies = self._build_cookies()
        
        self.session.headers.update(self.headers)
        self.session.cookies.update(self.cookies)
        if self.proxies:
            self.session.proxies.update(self.proxies)
        
        self.timeout = self.config['api']['timeout']
        
        print("ğŸš€ Solscan ä»£å¸æµåŠ¨åˆ†æå™¨å·²åˆå§‹åŒ–")
        print(f"ğŸ”§ ä»£ç†çŠ¶æ€: {'å¯ç”¨' if self.config['proxy']['enabled'] else 'ç¦ç”¨'}")
        print(f"ğŸ›¡ï¸ è‡ªåŠ¨æ›´æ–°: å¯ç”¨")
    
    def _load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"âŒ é…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°")
            raise
        except yaml.YAMLError as e:
            print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            raise
    
    def _save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _build_headers(self):
        """æ„å»ºè¯·æ±‚å¤´"""
        headers_config = self.config['headers']
        return {
            'accept': headers_config['accept'],
            'accept-encoding': headers_config['accept_encoding'],
            'accept-language': headers_config['accept_language'],
            'origin': headers_config['origin'],
            'referer': headers_config['referer'],
            'sec-ch-ua': headers_config['sec_ch_ua'],
            'sec-ch-ua-mobile': headers_config['sec_ch_ua_mobile'],
            'sec-ch-ua-platform': headers_config['sec_ch_ua_platform'],
            'sec-fetch-dest': headers_config['sec_fetch_dest'],
            'sec-fetch-mode': headers_config['sec_fetch_mode'],
            'sec-fetch-site': headers_config['sec_fetch_site'],
            'user-agent': headers_config['user_agent']
        }
    
    def _build_cookies(self):
        """æ„å»ºcookies"""
        cookies_config = self.config['cookies']
        return {
            '_ga': cookies_config['_ga'],
            'auth-token': cookies_config['auth_token'],
            'cf_clearance': cookies_config['cf_clearance'],
            '_ga_PS3V7B7KV0': cookies_config['_ga_PS3V7B7KV0']
        }
    
    def _update_cf_clearance_with_selenium(self):
        """ä½¿ç”¨ Selenium è‡ªåŠ¨è·å–æ–°çš„ cf_clearance"""
        try:
            import undetected_chromedriver as uc
            from selenium.webdriver.support.ui import WebDriverWait
            
            print("ğŸ”„ å¯åŠ¨æµè§ˆå™¨è·å–æ–°çš„ cf_clearance...")
            
            # é…ç½®æµè§ˆå™¨é€‰é¡¹
            options = uc.ChromeOptions()
            
            if self.config['proxy']['enabled']:
                proxy = self.config['proxy']['http'].replace('http://', '')
                options.add_argument(f'--proxy-server={proxy}')
            
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-gpu')
            
            # åˆ›å»ºæµè§ˆå™¨å®ä¾‹
            driver = uc.Chrome(options=options)
            
            try:
                # è®¿é—® solscan.io
                print("ğŸŒ æ­£åœ¨è®¿é—® solscan.io...")
                driver.get("https://solscan.io/")
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                print("â³ ç­‰å¾… Cloudflare éªŒè¯é€šè¿‡...")
                wait = WebDriverWait(driver, 60)
                wait.until(lambda driver: "solscan" in driver.title.lower())
                time.sleep(5)
                
                # è·å– cookies
                cookies = driver.get_cookies()
                cf_clearance = None
                other_cookies = {}
                
                for cookie in cookies:
                    if cookie['name'] == 'cf_clearance':
                        cf_clearance = cookie['value']
                    elif cookie['name'] in ['_ga', '_ga_PS3V7B7KV0']:
                        other_cookies[cookie['name']] = cookie['value']
                
                if cf_clearance:
                    # æ›´æ–°é…ç½®
                    self.config['cookies']['cf_clearance'] = cf_clearance
                    for name, value in other_cookies.items():
                        if name in self.config['cookies']:
                            self.config['cookies'][name] = value
                    
                    self._save_config()
                    
                    # æ›´æ–°å½“å‰ä¼šè¯
                    self.cookies['cf_clearance'] = cf_clearance
                    self.session.cookies.update({'cf_clearance': cf_clearance})
                    for name, value in other_cookies.items():
                        if name in self.cookies:
                            self.cookies[name] = value
                            self.session.cookies.update({name: value})
                    
                    self.cf_clearance_updated = True
                    print(f"âœ… cf_clearance å·²æ›´æ–°: {cf_clearance[:50]}...")
                    return True
                else:
                    print("âŒ æœªè·å–åˆ° cf_clearance")
                    return False
                    
            finally:
                driver.quit()
                
        except ImportError:
            print("âŒ éœ€è¦å®‰è£…ä¾èµ–: pip install undetected-chromedriver selenium")
            return False
        except Exception as e:
            print(f"âŒ Selenium æ›´æ–°å¤±è´¥: {str(e)}")
            return False
    
    def _update_cf_clearance_with_requests(self):
        """ä½¿ç”¨ HTTP è¯·æ±‚å°è¯•è·å–æ–°çš„ cf_clearance"""
        try:
            print("ğŸ”„ å°è¯• HTTP æ–¹å¼è·å– cf_clearance...")
            
            temp_session = requests.Session()
            temp_session.headers.update({
                'User-Agent': self.config['headers']['user_agent'],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            })
            
            if self.proxies:
                temp_session.proxies.update(self.proxies)
            
            response = temp_session.get("https://solscan.io/", timeout=30, verify=False)
            
            if 'cf_clearance' in temp_session.cookies:
                new_cf_clearance = temp_session.cookies['cf_clearance']
                
                self.config['cookies']['cf_clearance'] = new_cf_clearance
                self._save_config()
                
                self.cookies['cf_clearance'] = new_cf_clearance
                self.session.cookies.update({'cf_clearance': new_cf_clearance})
                
                self.cf_clearance_updated = True
                print(f"âœ… HTTP æ–¹å¼æ›´æ–°æˆåŠŸ: {new_cf_clearance[:50]}...")
                return True
            else:
                print("âŒ HTTP æ–¹å¼æ— æ³•è·å– cf_clearance")
                return False
                
        except Exception as e:
            print(f"âŒ HTTP æ›´æ–°å¤±è´¥: {str(e)}")
            return False
    
    def _handle_cloudflare_challenge(self, response):
        """å¤„ç† Cloudflare æŒ‘æˆ˜"""
        if response.status_code == 403 or "cloudflare" in response.text.lower():
            print("ğŸ›¡ï¸ æ£€æµ‹åˆ° Cloudflare é˜²æŠ¤ï¼Œå¼€å§‹è‡ªåŠ¨æ›´æ–°...")
            
            # å°è¯• Selenium æ–¹å¼
            if self._update_cf_clearance_with_selenium():
                return True
            
            # å°è¯• HTTP æ–¹å¼
            if self._update_cf_clearance_with_requests():
                return True
            
            print("âŒ æ‰€æœ‰è‡ªåŠ¨æ›´æ–°æ–¹å¼éƒ½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ‰‹åŠ¨æ›´æ–°")
            return False
        
        return True
    
    def get_token_transfers(self, address, page=1, page_size=None, from_time=None, to_time=None, value_filter=None):
        """
        è·å–ä»£å¸è½¬è´¦è®°å½•
        
        Args:
            address: ä»£å¸åœ°å€
            page: é¡µç 
            page_size: æ¯é¡µå¤§å°
            from_time: å¼€å§‹æ—¶é—´æˆ³
            to_time: ç»“æŸæ—¶é—´æˆ³
            value_filter: å€¼ç­›é€‰
        """
        url = f"{self.base_url}/v2/token/transfer"
        
        default_params = self.config['default_params']
        
        params = {
            'address': address,
            'page': page,
            'page_size': page_size or default_params['page_size'],
            'remove_spam': str(default_params['remove_spam']).lower(),
            'exclude_amount_zero': str(default_params['exclude_amount_zero']).lower()
        }
        
        if from_time:
            params['from_time'] = from_time
        if to_time:
            params['to_time'] = to_time
        if value_filter:
            params['value[]'] = value_filter
        
        max_retries = 2
        for attempt in range(max_retries + 1):
            try:
                print(f"ğŸ“¡ è¯·æ±‚ç¬¬ {page} é¡µæ•°æ® (å°è¯• {attempt + 1}/{max_retries + 1})")
                
                response = self.session.get(url, params=params, timeout=self.timeout, verify=False)
                
                # æ£€æŸ¥ Cloudflare æŒ‘æˆ˜
                if response.status_code == 403 or (response.status_code != 200 and "cloudflare" in response.text.lower()):
                    if attempt < max_retries:
                        if self._handle_cloudflare_challenge(response):
                            print("ğŸ”„ cf_clearance å·²æ›´æ–°ï¼Œé‡è¯•è¯·æ±‚...")
                            continue
                        else:
                            return None
                    else:
                        print("âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                        return None
                
                if response.status_code == 200:
                    data = response.json()
                    return data
                elif response.status_code == 304:
                    return {"message": "æ•°æ®æœªä¿®æ”¹", "status": 304}
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                    if attempt < max_retries:
                        print("ğŸ”„ ç¨åé‡è¯•...")
                        time.sleep(2)
                        continue
                    return None
                    
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
                if attempt < max_retries:
                    print("ğŸ”„ ç¨åé‡è¯•...")
                    time.sleep(2)
                    continue
                return None
        
        return None
    
    def crawl_all_data(self, address, from_time=None, to_time=None, value_filter=None, max_pages=None):
        """
        çˆ¬å–æ‰€æœ‰ä»£å¸è½¬è´¦æ•°æ®
        
        Args:
            address: ä»£å¸åœ°å€
            from_time: å¼€å§‹æ—¶é—´æˆ³
            to_time: ç»“æŸæ—¶é—´æˆ³
            value_filter: å€¼ç­›é€‰
            max_pages: æœ€å¤§é¡µæ•°
        
        Returns:
            dict: åŒ…å«æ‰€æœ‰æ•°æ®çš„ç»“æœ
        """
        all_data = []
        all_metadata = {}
        page = 1
        total_records = 0
        failed_pages = []
        
        # è·å–é…ç½®
        pagination_config = self.config.get('pagination', {})
        max_pages = max_pages or pagination_config.get('max_pages', 100)
        delay_between_pages = pagination_config.get('delay_between_pages', 0.5)
        retry_failed_pages = pagination_config.get('retry_failed_pages', 2)
        
        print("ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–æ•°æ®...")
        print(f"ğŸ“„ æœ€å¤§é¡µæ•°: {max_pages}")
        print(f"â±ï¸  é¡µé¢å»¶è¿Ÿ: {delay_between_pages}ç§’")
        
        start_time = datetime.now()
        
        while page <= max_pages:
            print(f"\nğŸ“– æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
            
            # é‡è¯•æœºåˆ¶
            page_data = None
            for retry in range(retry_failed_pages + 1):
                if retry > 0:
                    print(f"ğŸ”„ ç¬¬ {page} é¡µé‡è¯• {retry}/{retry_failed_pages}...")
                    time.sleep(delay_between_pages * 2)
                
                page_data = self.get_token_transfers(
                    address=address,
                    page=page,
                    from_time=from_time,
                    to_time=to_time,
                    value_filter=value_filter
                )
                
                if page_data:
                    break
            
            if not page_data:
                print(f"âŒ ç¬¬ {page} é¡µè·å–å¤±è´¥")
                failed_pages.append(page)
                page += 1
                continue
            
            # æ£€æŸ¥æ•°æ®
            if 'data' not in page_data or not page_data['data']:
                print(f"âœ… ç¬¬ {page} é¡µæ— æ›´å¤šæ•°æ®ï¼Œçˆ¬å–å®Œæˆ")
                break
            
            # ç´¯ç§¯æ•°æ®
            current_page_count = len(page_data['data'])
            all_data.extend(page_data['data'])
            total_records += current_page_count
            
            if 'metadata' in page_data:
                all_metadata = page_data['metadata']
            
            print(f"âœ… ç¬¬ {page} é¡µæˆåŠŸï¼Œæœ¬é¡µ {current_page_count} æ¡ï¼Œæ€»è®¡ {total_records} æ¡")
            
            # æ£€æŸ¥æ˜¯å¦æœ€åä¸€é¡µ
            default_page_size = self.config['default_params']['page_size']
            if current_page_count < default_page_size:
                print(f"ğŸ“„ æ•°æ®é‡å°äºé¡µå¤§å°ï¼Œå¯èƒ½æ˜¯æœ€åä¸€é¡µ")
                page += 1
                break
            
            page += 1
            
            # å»¶è¿Ÿ
            if page <= max_pages:
                time.sleep(delay_between_pages)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # æ„å»ºç»“æœ
        result = {
            "success": True,
            "total_pages": page - 1,
            "total_records": total_records,
            "failed_pages": failed_pages,
            "data": all_data,
            "metadata": all_metadata,
            "crawl_info": {
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "address": address,
                "from_time": from_time,
                "to_time": to_time,
                "value_filter": value_filter,
                "max_pages_limit": max_pages,
                "actual_pages": page - 1,
                "cf_clearance_updated": self.cf_clearance_updated
            }
        }
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡: {page - 1} é¡µ, {total_records} æ¡è®°å½•")
        print(f"â±ï¸  è€—æ—¶: {duration:.2f} ç§’")
        if failed_pages:
            print(f"âš ï¸ å¤±è´¥é¡µé¢: {failed_pages}")
        if self.cf_clearance_updated:
            print("ğŸ”„ æœ¬æ¬¡çˆ¬å–ä¸­ cf_clearance å·²è‡ªåŠ¨æ›´æ–°")
        
        return result
    
    def analyze_data(self, data):
        """
        åˆ†æä»£å¸è½¬è´¦æ•°æ®
        
        Args:
            data: çˆ¬å–çš„åŸå§‹æ•°æ®
            
        Returns:
            dict: åˆ†æç»“æœ
        """
        if not data or not data.get('data'):
            print("âŒ æ²¡æœ‰æ•°æ®å¯åˆ†æ")
            return None
        
        print("\nğŸ“Š å¼€å§‹åˆ†ææ•°æ®...")
        
        # è½¬æ¢ä¸º DataFrame
        df = pd.DataFrame(data['data'])
        
        # åŸºç¡€ç»Ÿè®¡
        total_transactions = len(df)
        
        # é‡‘é¢ç»Ÿè®¡
        if 'amount' in df.columns:
            df['amount'] = pd.to_numeric(df['amount'], errors='coerce')
            total_amount = df['amount'].sum()
            avg_amount = df['amount'].mean()
            median_amount = df['amount'].median()
            max_amount = df['amount'].max()
            min_amount = df['amount'].min()
        else:
            total_amount = avg_amount = median_amount = max_amount = min_amount = 0
        
        # ä»·å€¼ç»Ÿè®¡
        if 'value' in df.columns:
            df['value'] = pd.to_numeric(df['value'], errors='coerce')
            total_value = df['value'].sum()
            avg_value = df['value'].mean()
            median_value = df['value'].median()
            max_value = df['value'].max()
            min_value = df['value'].min()
        else:
            total_value = avg_value = median_value = max_value = min_value = 0
        
        # æ—¶é—´åˆ†æ
        if 'block_time' in df.columns:
            df['block_time'] = pd.to_datetime(df['block_time'], unit='s', errors='coerce')
            time_range = {
                'start': df['block_time'].min(),
                'end': df['block_time'].max(),
                'span_hours': (df['block_time'].max() - df['block_time'].min()).total_seconds() / 3600
            }
        else:
            time_range = None
        
        # åœ°å€åˆ†æ
        unique_from = df['from'].nunique() if 'from' in df.columns else 0
        unique_to = df['to'].nunique() if 'to' in df.columns else 0
        
        # æ„å»ºåˆ†æç»“æœ
        analysis = {
            "summary": {
                "total_transactions": total_transactions,
                "unique_from_addresses": unique_from,
                "unique_to_addresses": unique_to,
                "analysis_time": datetime.now().isoformat()
            },
            "amount_stats": {
                "total": total_amount,
                "average": avg_amount,
                "median": median_amount,
                "max": max_amount,
                "min": min_amount
            },
            "value_stats": {
                "total_usd": total_value,
                "average_usd": avg_value,
                "median_usd": median_value,
                "max_usd": max_value,
                "min_usd": min_value
            },
            "time_analysis": time_range,
            "raw_data_info": {
                "total_pages": data.get('total_pages', 0),
                "crawl_duration": data.get('crawl_info', {}).get('duration_seconds', 0),
                "cf_updated": data.get('crawl_info', {}).get('cf_clearance_updated', False)
            }
        }
        
        # æ‰“å°åˆ†æç»“æœ
        print("="*60)
        print("ğŸ“ˆ æ•°æ®åˆ†æç»“æœ")
        print("="*60)
        print(f"ğŸ“ æ€»äº¤æ˜“æ•°: {total_transactions:,}")
        print(f"ğŸ“¤ å”¯ä¸€å‘é€åœ°å€: {unique_from:,}")
        print(f"ğŸ“¥ å”¯ä¸€æ¥æ”¶åœ°å€: {unique_to:,}")
        print(f"ğŸ’° æ€»ä»£å¸æ•°é‡: {total_amount:,.2f}")
        print(f"ğŸ’µ æ€»ä»·å€¼: ${total_value:,.2f}")
        print(f"ğŸ“Š å¹³å‡äº¤æ˜“ä»·å€¼: ${avg_value:.2f}")
        print(f"ğŸ“Š ä¸­ä½æ•°äº¤æ˜“ä»·å€¼: ${median_value:.2f}")
        
        if time_range:
            print(f"â° æ—¶é—´è·¨åº¦: {time_range['span_hours']:.2f} å°æ—¶")
            print(f"ğŸ• å¼€å§‹æ—¶é—´: {time_range['start']}")
            print(f"ğŸ• ç»“æŸæ—¶é—´: {time_range['end']}")
        
        return analysis
    
    def save_data(self, data, filename=None, include_analysis=True):
        """
        ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
        
        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            include_analysis: æ˜¯å¦åŒ…å«åˆ†æç»“æœ
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not filename:
            storage_config = self.config['storage']
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename_template = storage_config['filename_format']
            filename = os.path.join(
                storage_config['directory'], 
                filename_template.format(timestamp=timestamp)
            )
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        try:
            # å¦‚æœéœ€è¦åŒ…å«åˆ†æç»“æœ
            if include_analysis and data.get('data'):
                analysis = self.analyze_data(data)
                if analysis:
                    data['analysis'] = analysis
            
            # ä¿å­˜ JSON æ–‡ä»¶
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            
            # åŒæ—¶ä¿å­˜ CSV æ ¼å¼ï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
            if data.get('data'):
                csv_filename = filename.replace('.json', '.csv')
                df = pd.DataFrame(data['data'])
                df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
                print(f"âœ… CSV æ ¼å¼å·²ä¿å­˜åˆ°: {csv_filename}")
            
            return filename
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
            return None
    
    def run_analysis(self, token_address=None, from_time=None, to_time=None, value_filter=None, max_pages=None):
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        Args:
            token_address: ä»£å¸åœ°å€ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼ï¼‰
            from_time: å¼€å§‹æ—¶é—´æˆ³
            to_time: ç»“æŸæ—¶é—´æˆ³
            value_filter: å€¼ç­›é€‰
            max_pages: æœ€å¤§é¡µæ•°
        """
        # è·å–ä»£å¸ä¿¡æ¯
        if not token_address:
            target_tokens = self.config.get('target_tokens', [])
            if target_tokens:
                token_info = target_tokens[0]
                token_address = token_info['address']
                token_name = token_info.get('name', 'Unknown')
                token_symbol = token_info.get('symbol', 'Unknown')
            else:
                print("âŒ æœªæŒ‡å®šä»£å¸åœ°å€ä¸”é…ç½®æ–‡ä»¶ä¸­æ— é»˜è®¤ä»£å¸")
                return None
        else:
            token_name = "Custom"
            token_symbol = "Custom"
        
        print("ğŸ¯ å¼€å§‹ä»£å¸æµåŠ¨åˆ†æ...")
        print(f"ğŸ“ ä»£å¸åœ°å€: {token_address}")
        print(f"ğŸ·ï¸  ä»£å¸åç§°: {token_name}")
        print(f"ğŸ”¤ ä»£å¸ç¬¦å·: {token_symbol}")
        if from_time:
            print(f"â° å¼€å§‹æ—¶é—´: {datetime.fromtimestamp(from_time)}")
        if to_time:
            print(f"â° ç»“æŸæ—¶é—´: {datetime.fromtimestamp(to_time)}")
        if value_filter:
            print(f"ğŸ’° å€¼ç­›é€‰: >= ${value_filter}")
        print("=" * 60)
        
        # çˆ¬å–æ•°æ®
        data = self.crawl_all_data(
            address=token_address,
            from_time=from_time,
            to_time=to_time,
            value_filter=value_filter,
            max_pages=max_pages
        )
        
        if not data or not data.get('total_records'):
            print("âŒ æœªèƒ½è·å–åˆ°æ•°æ®")
            return None
        
        # ä¿å­˜æ•°æ®ï¼ˆåŒ…å«åˆ†æï¼‰
        print("\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®å’Œåˆ†æç»“æœ...")
        saved_file = self.save_data(data, include_analysis=True)
        
        if saved_file:
            print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
            print(f"ğŸ“ ç»“æœæ–‡ä»¶: {saved_file}")
            print(f"ğŸ“Š æ•°æ®è®°å½•: {data['total_records']:,} æ¡")
            print(f"â±ï¸  æ€»è€—æ—¶: {data['crawl_info'].get('duration_seconds', 0):.2f} ç§’")
            
            return {
                'data': data,
                'file': saved_file,
                'success': True
            }
        else:
            print("âŒ ä¿å­˜å¤±è´¥")
            return None

def main():
    """ä¸»å‡½æ•° - è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
    print("ğŸŒŸ Solscan Token Flow Analyzer v2.0")
    print("=" * 60)
    
    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer = SolscanAnalyzer()
        
        # è¿è¡Œåˆ†æ
        result = analyzer.run_analysis(
            # å¯ä»¥åœ¨è¿™é‡Œè‡ªå®šä¹‰å‚æ•°
            from_time=1756544400,  # å¯é€‰
            to_time=1756548000,    # å¯é€‰
            value_filter=30,       # å¯é€‰
            max_pages=50          # å¯é€‰
        )
        
        if result and result['success']:
            print("\nğŸŠ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        else:
            print("\nâŒ åˆ†æå¤±è´¥")
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
