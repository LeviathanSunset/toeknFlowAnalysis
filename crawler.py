import requests
import json
import time
import yaml
import os
from datetime import datetime
from pathlib import Path
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class SolscanCrawler:
    def __init__(self, config_path="settings/config.yaml"):
        self.config = self._load_config(config_path)
        self.base_url = self.config['api']['base_url']
        self.session = requests.Session()
        
        # è®¾ç½®ä»£ç†
        if self.config['proxy']['enabled']:
            self.proxies = {
                'http': self.config['proxy']['http'],
                'https': self.config['proxy']['https']
            }
        else:
            self.proxies = {}
        
        # è®¾ç½®é‡è¯•ç­–ç•¥
        retry_config = self.config['retry']
        retry_strategy = Retry(
            total=retry_config['max_retries'],
            backoff_factor=retry_config['backoff_factor'],
            status_forcelist=retry_config['status_codes'],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = self._build_headers()
        
        # è®¾ç½®cookies
        self.cookies = self._build_cookies()
        
        self.session.headers.update(self.headers)
        self.session.cookies.update(self.cookies)
        if self.proxies:
            self.session.proxies.update(self.proxies)
        
        # è®¾ç½®è¶…æ—¶æ—¶é—´
        self.timeout = self.config['api']['timeout']
    
    def _load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"é…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°")
            raise
        except yaml.YAMLError as e:
            print(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            raise
    
    def _build_headers(self):
        """æ„å»ºè¯·æ±‚å¤´"""
        headers_config = self.config['headers']
        return {
            'accept': headers_config['accept'],
            'accept-encoding': headers_config['accept_encoding'],
            'accept-language': headers_config['accept_language'],
            'origin': headers_config['origin'],
            'referer': headers_config['referer'],
            'sec-ch-ua': headers_config['sec_ch_ua'],
            'sec-ch-ua-mobile': headers_config['sec_ch_ua_mobile'],
            'sec-ch-ua-platform': headers_config['sec_ch_ua_platform'],
            'sec-fetch-dest': headers_config['sec_fetch_dest'],
            'sec-fetch-mode': headers_config['sec_fetch_mode'],
            'sec-fetch-site': headers_config['sec_fetch_site'],
            'user-agent': headers_config['user_agent']
        }
    
    def _build_cookies(self):
        """æ„å»ºcookies"""
        cookies_config = self.config['cookies']
        return {
            '_ga': cookies_config['_ga'],
            'auth-token': cookies_config['auth_token'],
            'cf_clearance': cookies_config['cf_clearance'],
            '_ga_PS3V7B7KV0': cookies_config['_ga_PS3V7B7KV0']
        }
    
    def get_token_transfers(self, address, page=1, page_size=None, from_time=None, to_time=None, value_filter=None):
        """
        è·å–ä»£å¸è½¬è´¦è®°å½•
        
        Args:
            address: ä»£å¸åœ°å€
            page: é¡µç 
            page_size: æ¯é¡µå¤§å° (å¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼)
            from_time: å¼€å§‹æ—¶é—´æˆ³
            to_time: ç»“æŸæ—¶é—´æˆ³
            value_filter: å€¼ç­›é€‰
        """
        url = f"{self.base_url}/v2/token/transfer"
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å‚æ•°
        default_params = self.config['default_params']
        
        params = {
            'address': address,
            'page': page,
            'page_size': page_size or default_params['page_size'],
            'remove_spam': str(default_params['remove_spam']).lower(),
            'exclude_amount_zero': str(default_params['exclude_amount_zero']).lower()
        }
        
        if from_time:
            params['from_time'] = from_time
        if to_time:
            params['to_time'] = to_time
        if value_filter:
            params['value[]'] = value_filter
            
        try:
            print(f"æ­£åœ¨è¯·æ±‚: {url}")
            print(f"å‚æ•°: {params}")
            if self.proxies:
                print(f"ä½¿ç”¨ä»£ç†: {self.proxies}")
            
            response = self.session.get(url, params=params, timeout=self.timeout, verify=False)
            
            print(f"çŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å¤´: {dict(response.headers)}")
            
            if response.status_code == 200:
                data = response.json()
                print("è¯·æ±‚æˆåŠŸ!")
                return data
            elif response.status_code == 304:
                print("æ•°æ®æœªä¿®æ”¹ (304)")
                return {"message": "æ•°æ®æœªä¿®æ”¹", "status": 304}
            else:
                print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                print(f"å“åº”å†…å®¹: {response.text}")
                return None
                
        except Exception as e:
            print(f"è¯·æ±‚å‡ºé”™: {str(e)}")
            return None
    
    def get_all_token_transfers(self, address, from_time=None, to_time=None, value_filter=None, max_pages=None):
        """
        è·å–æ‰€æœ‰ä»£å¸è½¬è´¦è®°å½•ï¼Œè‡ªåŠ¨ç¿»é¡µç›´åˆ°æ²¡æœ‰æ›´å¤šæ•°æ®
        
        Args:
            address: ä»£å¸åœ°å€
            from_time: å¼€å§‹æ—¶é—´æˆ³
            to_time: ç»“æŸæ—¶é—´æˆ³
            value_filter: å€¼ç­›é€‰
            max_pages: æœ€å¤§é¡µæ•°é™åˆ¶ï¼ˆä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶ï¼‰
        
        Returns:
            dict: åŒ…å«æ‰€æœ‰æ•°æ®çš„æ±‡æ€»ç»“æœ
        """
        all_data = []
        all_metadata = {}
        page = 1
        total_records = 0
        failed_pages = []
        
        # è·å–ç¿»é¡µé…ç½®
        pagination_config = self.config.get('pagination', {})
        max_pages = max_pages or pagination_config.get('max_pages', 100)
        delay_between_pages = pagination_config.get('delay_between_pages', 0.5)
        retry_failed_pages = pagination_config.get('retry_failed_pages', 2)
        
        print("ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–æ•°æ®...")
        print(f"ğŸ“„ æœ€å¤§é¡µæ•°: {max_pages}")
        print(f"â±ï¸  é¡µé¢å»¶è¿Ÿ: {delay_between_pages}ç§’")
        
        while page <= max_pages:
            print(f"\nğŸ“– æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
            
            # é‡è¯•æœºåˆ¶
            page_data = None
            for retry in range(retry_failed_pages + 1):
                if retry > 0:
                    print(f"ï¿½ ç¬¬ {page} é¡µé‡è¯• {retry}/{retry_failed_pages}...")
                    time.sleep(delay_between_pages * 2)  # é‡è¯•æ—¶å»¶è¿Ÿæ›´ä¹…
                
                # è·å–å½“å‰é¡µæ•°æ®
                page_data = self.get_token_transfers(
                    address=address,
                    page=page,
                    from_time=from_time,
                    to_time=to_time,
                    value_filter=value_filter
                )
                
                if page_data:
                    break
            
            if not page_data:
                print(f"âŒ ç¬¬ {page} é¡µè·å–å¤±è´¥ï¼ˆå·²é‡è¯• {retry_failed_pages} æ¬¡ï¼‰")
                failed_pages.append(page)
                page += 1
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            if 'data' not in page_data or not page_data['data']:
                print(f"âœ… ç¬¬ {page} é¡µæ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œçˆ¬å–å®Œæˆ")
                break
            
            # ç´¯ç§¯æ•°æ®
            current_page_count = len(page_data['data'])
            all_data.extend(page_data['data'])
            total_records += current_page_count
            
            # ä¿å­˜å…ƒæ•°æ®ï¼ˆä½¿ç”¨æœ€æ–°çš„ï¼‰
            if 'metadata' in page_data:
                all_metadata = page_data['metadata']
            
            print(f"âœ… ç¬¬ {page} é¡µè·å–æˆåŠŸï¼Œæœ¬é¡µ {current_page_count} æ¡è®°å½•ï¼Œæ€»è®¡ {total_records} æ¡")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€åä¸€é¡µï¼ˆæ•°æ®é‡å°‘äºé»˜è®¤é¡µå¤§å°ï¼‰
            default_page_size = self.config['default_params']['page_size']
            if current_page_count < default_page_size:
                print(f"ğŸ“„ ç¬¬ {page} é¡µæ•°æ®é‡ ({current_page_count}) å°äºé¡µå¤§å° ({default_page_size})ï¼Œå¯èƒ½æ˜¯æœ€åä¸€é¡µ")
                break
            
            page += 1
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            if page <= max_pages:
                time.sleep(delay_between_pages)
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        result = {
            "success": True,
            "total_pages": page - 1,
            "total_records": total_records,
            "failed_pages": failed_pages,
            "data": all_data,
            "metadata": all_metadata,
            "crawl_info": {
                "start_time": datetime.now().isoformat(),
                "address": address,
                "from_time": from_time,
                "to_time": to_time,
                "value_filter": value_filter,
                "max_pages_limit": max_pages,
                "actual_pages": page - 1
            }
        }
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡çˆ¬å– {page-1} é¡µï¼Œ{total_records} æ¡è®°å½•")
        if failed_pages:
            print(f"âš ï¸ å¤±è´¥é¡µé¢: {failed_pages}")
        
        return result

    def save_to_file(self, data, filename=None):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        if not filename:
            storage_config = self.config['storage']
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename_template = storage_config['filename_format']
            filename = os.path.join(
                storage_config['directory'], 
                filename_template.format(timestamp=timestamp)
            )
        
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None

def main():
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = SolscanCrawler()
    
    # ä»é…ç½®æ–‡ä»¶è·å–ç›®æ ‡ä»£å¸ä¿¡æ¯
    target_tokens = crawler.config.get('target_tokens', [])
    if not target_tokens:
        print("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç›®æ ‡ä»£å¸ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        # ä½¿ç”¨åŸæ¥çš„ç¡¬ç¼–ç å‚æ•°ä½œä¸ºå¤‡é€‰
        address = "5zCETicUCJqJ5Z3wbfFPZqtSpHPYqnggs1wX7ZRpump"
        token_name = "SPARK"
    else:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç¬¬ä¸€ä¸ªä»£å¸
        token_info = target_tokens[0]
        address = token_info['address']
        token_name = token_info.get('name', 'Unknown')
    
    # æŸ¥è¯¢å‚æ•°
    from_time = 1756544400
    to_time = 1756548000
    value_filter = 30
    
    # ä»é…ç½®æ–‡ä»¶è·å–æœ€å¤§é¡µæ•°ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
    max_pages = crawler.config.get('pagination', {}).get('max_pages', 100)
    
    print("ğŸ¯ å¼€å§‹çˆ¬å– Solscan æ•°æ®...")
    print(f"ğŸ“ ä»£å¸åœ°å€: {address}")
    print(f"ğŸ·ï¸  ä»£å¸åç§°: {token_name}")
    print(f"â° æ—¶é—´èŒƒå›´: {from_time} - {to_time}")
    print(f"ğŸ’° å€¼ç­›é€‰: >= ${value_filter}")
    print(f"ğŸ“„ æœ€å¤§é¡µæ•°: {max_pages}")
    print("=" * 60)
    
    # è·å–æ‰€æœ‰æ•°æ®
    all_data = crawler.get_all_token_transfers(
        address=address,
        from_time=from_time,
        to_time=to_time,
        value_filter=value_filter
    )
    
    if all_data and all_data.get('total_records', 0) > 0:
        print("\n" + "="*60)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡ä¿¡æ¯:")
        print("="*60)
        print(f"ğŸ“„ æ€»é¡µæ•°: {all_data['total_pages']}")
        print(f"ğŸ“ æ€»è®°å½•æ•°: {all_data['total_records']}")
        print(f"ğŸ“ ä»£å¸åœ°å€: {all_data['crawl_info']['address']}")
        print(f"â° çˆ¬å–æ—¶é—´: {all_data['crawl_info']['start_time']}")
        print(f"ğŸ¯ å®é™…é¡µæ•°: {all_data['crawl_info']['actual_pages']}")
        
        if all_data.get('failed_pages'):
            print(f"âš ï¸ å¤±è´¥é¡µé¢: {all_data['failed_pages']}")
        
        # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®ä½œä¸ºé¢„è§ˆ
        print(f"\nğŸ“‹ æ•°æ®é¢„è§ˆ (å‰5æ¡):")
        print("-" * 40)
        for i, record in enumerate(all_data['data'][:5]):
            print(f"{i+1}. äº¤æ˜“ID: {record.get('trans_id', 'N/A')[:20]}...")
            print(f"   é‡‘é¢: {record.get('amount', 0):,}")
            print(f"   ä»·å€¼: ${record.get('value', 0):.2f}")
            print(f"   æ—¶é—´: {record.get('block_time', 'N/A')}")
            print()
        
        # ä¿å­˜æ•°æ®
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®...")
        saved_file = crawler.save_to_file(all_data)
        if saved_file:
            print(f"\nâœ… æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°: {saved_file}")
            print(f"ğŸ“Š æ–‡ä»¶åŒ…å« {all_data['total_records']} æ¡è®°å½•")
        else:
            print("âŒ æ•°æ®ä¿å­˜å¤±è´¥")
    else:
        print("âŒ æœªèƒ½è·å–åˆ°ä»»ä½•æ•°æ®")

if __name__ == "__main__":
    main()
